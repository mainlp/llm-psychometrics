@inproceedings{Lalor2019,
  title     = {Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds},
  author    = {Lalor, John P.  and
               Wu, Hao  and
               Yu, Hong},
  editor    = {Inui, Kentaro  and
               Jiang, Jing  and
               Ng, Vincent  and
               Wan, Xiaojun},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1434/},
  doi       = {10.18653/v1/D19-1434},
  pages     = {4249--4259}
}

@inproceedings{Saeuberli2024,
  title     = {Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models},
  author    = {S{\"a}uberli, Andreas  and
               Clematide, Simon},
  editor    = {Wilkens, Rodrigo  and
               Cardon, R{\'e}mi  and
               Todirascu, Amalia  and
               Gala, N{\'u}ria},
  booktitle = {Proceedings of the 3rd Workshop on Tools and Resources for People with REAding DIfficulties (READI) @ LREC-COLING 2024},
  month     = may,
  year      = {2024},
  address   = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url       = {https://aclanthology.org/2024.readi-1.3/},
  pages     = {22--37}
}

@article{Hambleton1993,
  author    = {Hambleton, Ronald K. and Jones, Russell W.},
  journal   = {Educational Measurement: Issues and Practice},
  title     = {An {NCME} Instructional Module on: Comparison of Classical Test Theory and Item Response Theory and Their Applications to Test Development},
  year      = {1993},
  issn      = {1745-3992},
  month     = sep,
  number    = {3},
  pages     = {38--47},
  volume    = {12},
  doi       = {10.1111/j.1745-3992.1993.tb00543.x},
  publisher = {Wiley}
}

@article{Fan1998,
  author    = {Fan, Xitao},
  journal   = {Educational and Psychological Measurement},
  title     = {Item Response Theory and Classical Test Theory: An Empirical Comparison of their Item/Person Statistics},
  year      = {1998},
  issn      = {1552-3888},
  month     = jun,
  number    = {3},
  pages     = {357--381},
  volume    = {58},
  doi       = {10.1177/0013164498058003001},
  publisher = {SAGE Publications}
}

@inproceedings{Chen2024,
  title     = {{\textquotedblleft}Seeing the Big through the Small{\textquotedblright}: Can {LLM}s Approximate Human Judgment Distributions on {NLI} from a Few Explanations?},
  author    = {Chen, Beiduo  and
               Wang, Xinpeng  and
               Peng, Siyao  and
               Litschko, Robert  and
               Korhonen, Anna  and
               Plank, Barbara},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-emnlp.842/},
  doi       = {10.18653/v1/2024.findings-emnlp.842},
  pages     = {14396--14419}
}

@inproceedings{Yang2024,
  title     = {Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?},
  author    = {Yang, Zhe  and
               Zhang, Yichang  and
               Liu, Tianyu  and
               Yang, Jian  and
               Lin, Junyang  and
               Zhou, Chang  and
               Sui, Zhifang},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.92/},
  doi       = {10.18653/v1/2024.emnlp-main.92},
  pages     = {1531--1555}
}

@inproceedings{Baan2024,
  title     = {Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?},
  author    = {Baan, Joris  and
               Fern{\'a}ndez, Raquel  and
               Plank, Barbara  and
               Aziz, Wilker},
  editor    = {Graham, Yvette  and
               Purver, Matthew},
  booktitle = {Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = mar,
  year      = {2024},
  address   = {St. Julian{'}s, Malta},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.eacl-short.24/},
  pages     = {268--277}
}

@techreport{Mullooly2023,
  author    = {Mullooly, Andrew and Andersen, Øistein and Benedetto, Luca and Buttery, Paula and Caines, Andrew and Gales, Mark J. F. and Karatay, Yasin and Knill, Kate and Liusie, Adian and Raina, Vatsal and Taslimipoor, Shiva},
  title     = {The {Cambridge} {Multiple-Choice} {Questions} {Reading} {Dataset}},
  year      = {2023},
  copyright = {https://www.rioxx.net/licenses/all-rights-reserved/},
  doi       = {10.17863/CAM.102185},
  language  = {en},
  publisher = {Cambridge University Press and Assessment}
}

@inproceedings{Hayakawa2024,
  author    = {Akio Hayakawa and Horacio Saggion},
  booktitle = {Fourth Workshop on Knowledge-infused Learning},
  title     = {Can {LLM}s Solve Reading Comprehension Tests as Second Language Learners?},
  year      = {2024},
  url       = {https://openreview.net/forum?id=Gu47GKK85N}
}

@article{Gorgun2024,
  author    = {Gorgun, Guher and Bulut, Okan},
  journal   = {Educational Measurement: Issues and Practice},
  title     = {Instruction-Tuned Large-Language Models for Quality Control in Automatic Item Generation: A Feasibility Study},
  year      = {2024},
  issn      = {1745-3992},
  month     = dec,
  doi       = {10.1111/emip.12663},
  publisher = {Wiley}
}

@article{Liusie2023,
  author        = {Liusie, Adian and Raina, Vatsal and Mullooly, Andrew and Knill, Kate and Gales, Mark J. F.},
  title         = {Analysis of the {Cambridge} {Multiple-Choice} {Questions} {Reading} {Dataset} with a Focus on Candidate Response Distribution},
  year          = {2023},
  month         = jun,
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2306.13047},
  eprint        = {2306.13047},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@inproceedings{McCarthy2021,
  author    = {McCarthy, Arya D. and Yancey, Kevin P. and LaFlair, Geoff T. and Egbert, Jesse and Liao, Manqian and Settles, Burr},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  title     = {Jump-Starting Item Parameters for Adaptive Language Tests},
  year      = {2021},
  pages     = {883--899},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.emnlp-main.67}
}

@inproceedings{Yancey2024,
  author    = {Yancey, Kevin P. and Runge, Andrew and LaFlair, Geoffrey and Mulcaire, Phoebe},
  booktitle = {Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)},
  title     = {{BERT}-{IRT}: Accelerating Item Piloting with {BERT} Embeddings and Explainable {IRT} Models},
  year      = {2024},
  address   = {Mexico City, Mexico},
  editor    = {Kochmar, Ekaterina and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Ana{\"i}s and Yaneva, Victoria and Yuan, Zheng},
  month     = jun,
  pages     = {428--438},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.bea-1.35/}
}

@inproceedings{Zotos2025,
  author    = {Zotos, Leonidas and van Rijn, Hedderik and Nissim, Malvina},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  title     = {Can Model Uncertainty Function as a Proxy for Multiple-Choice Question Item Difficulty?},
  year      = {2025},
  address   = {Abu Dhabi, UAE},
  editor    = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  month     = jan,
  pages     = {11304--11316},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.coling-main.749/}
}

@inproceedings{Bulut2024,
  author    = {Bulut, Okan and Gorgun, Guher and Tan, Bin},
  booktitle = {Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)},
  title     = {Item Difficulty and Response Time Prediction with Large Language Models: An Empirical Analysis of {USMLE} Items},
  year      = {2024},
  address   = {Mexico City, Mexico},
  editor    = {Kochmar, Ekaterina and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Ana{\"\i}s and Yaneva, Victoria and Yuan, Zheng},
  month     = jun,
  pages     = {522--527},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.bea-1.44}
}

@article{Balepur2025,
  author        = {Balepur, Nishant and Rudinger, Rachel and Boyd-Graber, Jordan Lee},
  title         = {Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above},
  year          = {2025},
  month         = feb,
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2502.14127},
  eprint        = {2502.14127},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@inproceedings{Zheng2024,
  author    = {Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  title     = {Large Language Models Are Not Robust Multiple Choice Selectors},
  year      = {2024},
  url       = {https://openreview.net/forum?id=shr9PXz7T0}
}

@inproceedings{Guo2017,
  author    = {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {On Calibration of Modern Neural Networks},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  month     = {06--11 Aug},
  pages     = {1321--1330},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  url       = {https://proceedings.mlr.press/v70/guo17a.html}
}

@book{Papageorgiou2021,
  author     = {Papageorgiou, Spiros and Davis, Larry and Norris, John M. and Garcia Gomez, Pablo and Manna, Venessa F. and Monfils, Lora},
  publisher  = {Educational Testing Service},
  title      = {Design Framework for the TOEFL{\textregistered} Essentials{\texttrademark} Test 2021},
  year       = {2021},
  accessdate = {2024-03-06},
  url        = {https://www.ets.org/Media/Research/pdf/RM-21-03.pdf}
}

@incollection{Green2020,
  author    = {Green, Rita},
  booktitle = {The Routledge Handbook of Second Language Acquisition and Language Testing},
  publisher = {Routledge},
  title     = {Pilot Testing: Why and How We Trial},
  year      = {2020},
  chapter   = {11},
  isbn      = {9781351034784},
  pages     = {115--124}
}

@article{Owan2023,
  author    = {Owan, Valentine Joseph and Abang, Kinsgley Bekom and Idika, Delight Omoji and Etta, Eugene Onor and Bassey, Bassey Asuquo},
  journal   = {Eurasia Journal of Mathematics, Science and Technology Education},
  title     = {Exploring the potential of artificial intelligence tools in educational measurement and assessment},
  year      = {2023},
  issn      = {1305-8223},
  month     = aug,
  number    = {8},
  pages     = {em2307},
  volume    = {19},
  doi       = {10.29333/ejmste/13428},
  publisher = {Modestum Ltd}
}

@article{Attali2022,
  author    = {Yigal Attali and Andrew Runge and Geoffrey T. LaFlair and Kevin Yancey and Sarah Goodwin and Yena Park and Alina A. von Davier},
  journal   = {Frontiers in Artificial Intelligence},
  title     = {The interactive reading task: Transformer-based automatic item generation},
  year      = {2022},
  month     = jul,
  volume    = {5},
  doi       = {10.3389/frai.2022.903077},
  publisher = {Frontiers Media {SA}}
}

@incollection{Haladyna2013,
  author     = {Haladyna, Thomas M.},
  booktitle  = {Automatic Item Generation: Theory and Practice},
  publisher  = {Routledge},
  title      = {Automatic Item Generation: A Historical Perspective},
  year       = {2013},
  address    = {New York},
  chapter    = {2},
  editor     = {Gierl, Mark J. and Haladyna, Thomas M.},
  isbn       = {9780415897501},
  pages      = {13--25},
  shorttitle = {Automatic Item Generation}
}

@inproceedings{Yaneva2024,
  author    = {Yaneva, Victoria and North, Kai and Baldwin, Peter and Ha, Le An and Rezayi, Saed and Zhou, Yiyun and Ray Choudhury, Sagnik and Harik, Polina and Clauser, Brian},
  booktitle = {Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)},
  title     = {Findings from the First Shared Task on Automated Prediction of Difficulty and Response Time for Multiple-Choice Questions},
  year      = {2024},
  address   = {Mexico City, Mexico},
  editor    = {Kochmar, Ekaterina and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Ana{\"\i}s and Yaneva, Victoria and Yuan, Zheng},
  month     = jun,
  pages     = {470--482},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.bea-1.39}
}

@inproceedings{Baan2022,
  author    = {Baan, Joris and Aziz, Wilker and Plank, Barbara and Fernandez, Raquel},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  title     = {Stop Measuring Calibration When Humans Disagree},
  year      = {2022},
  pages     = {1892--1915},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.emnlp-main.124}
}

@article{May2025,
  author    = {May, Toni A. and Fan, Yiyun Kate and Stone, Gregory E. and Koskey, Kristin L. K. and Sondergeld, Connor J. and Folger, Timothy D. and Archer, James N. and Provinzano, Kathleen and Johnson, Carla C.},
  journal   = {Education Sciences},
  title     = {An Effectiveness Study of Generative Artificial Intelligence Tools Used to Develop Multiple-Choice Test Items},
  year      = {2025},
  issn      = {2227-7102},
  month     = jan,
  number    = {2},
  pages     = {144},
  volume    = {15},
  doi       = {10.3390/educsci15020144},
  publisher = {MDPI AG}
}

@incollection{Livingston2011,
  author    = {Livingston, Samuel A.},
  booktitle = {Handbook of Test Development},
  publisher = {Taylor \& Francis Group},
  title     = {Item analysis},
  year      = {2011},
  editor    = {Downing, Steven M. and Haladyna, Thomas M.},
  isbn      = {9780203874776},
  pages     = {421--441}
}

@article{Grattafiori2024,
  author        = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
  title         = {The {Llama} 3 Herd of Models},
  year          = {2024},
  month         = jul,
  abstract      = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2407.21783},
  eprint        = {2407.21783},
  keywords      = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.AI},
  publisher     = {arXiv}
}

@article{OLMo2025,
  author        = {OLMo, Team and Walsh, Pete and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia, Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt and Lambert, Nathan and Schwenk, Dustin and Tafjord, Oyvind and Anderson, Taira and Atkinson, David and Brahman, Faeze and Clark, Christopher and Dasigi, Pradeep and Dziri, Nouha and Guerquin, Michal and Ivison, Hamish and Koh, Pang Wei and Liu, Jiacheng and Malik, Saumya and Merrill, William and Miranda, Lester James V. and Morrison, Jacob and Murray, Tyler and Nam, Crystal and Pyatkin, Valentina and Rangapur, Aman and Schmitz, Michael and Skjonsberg, Sam and Wadden, David and Wilhelm, Christopher and Wilson, Michael and Zettlemoyer, Luke and Farhadi, Ali and Smith, Noah A. and Hajishirzi, Hannaneh},
  title         = {2 {OLMo} 2 {Furious}},
  year          = {2025},
  month         = dec,
  abstract      = {We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2501.00656},
  eprint        = {2501.00656},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Abdin2024,
  author        = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Martin and Cai, Qin and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng, Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao, Mei and Gao, Min and Garg, Amit and Del Giorno, Allie and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Hu, Wenxiang and Huynh, Jamie and Iter, Dan and Jacobs, Sam Ade and Javaheripi, Mojan and Jin, Xin and Karampatziakis, Nikos and Kauffmann, Piero and Khademi, Mahoud and Kim, Dongwoo and Kim, Young Jin and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Lin, Xihui and Lin, Zeqi and Liu, Ce and Liu, Liyuan and Liu, Mengchen and Liu, Weishung and Liu, Xiaodong and Luo, Chong and Madan, Piyush and Mahmoudzadeh, Ali and Majercak, David and Mazzola, Matt and Mendes, Caio César Teodoro and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Ren, Liliang and de Rosa, Gustavo and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shen, Yelong and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Vaddamanu, Praneetha and Wang, Chunyu and Wang, Guanhua and Wang, Lijuan and Wang, Shuohang and Wang, Xin and Wang, Yu and Ward, Rachel and Wen, Wen and Witte, Philipp and Wu, Haiping and Wu, Xiaoxia and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Xue, Jilong and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Yifan and Yang, Ziyi and Yu, Donghan and Yuan, Lu and Zhang, Chenruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
  title         = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  year          = {2024},
  month         = apr,
  abstract      = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2404.14219},
  eprint        = {2404.14219},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Abdin2024a,
  author        = {Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, Sébastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J. and Javaheripi, Mojan and Kauffmann, Piero and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liu, Weishung and Mendes, Caio C. T. and Nguyen, Anh and Price, Eric and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Wang, Xin and Ward, Rachel and Wu, Yue and Yu, Dingli and Zhang, Cyril and Zhang, Yi},
  title         = {Phi-4 Technical Report},
  year          = {2024},
  month         = dec,
  abstract      = {We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2412.08905},
  eprint        = {2412.08905},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@article{Qwen2024,
  author        = {Qwen, Team and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
  title         = {Qwen2.5 Technical Report},
  year          = {2024},
  month         = dec,
  abstract      = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2412.15115},
  eprint        = {2412.15115},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@inproceedings{Wolf2020,
  author    = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  year      = {2020},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.emnlp-demos.6}
}

@inproceedings{Wang2024,
  author    = {Wang, Xinpeng and Ma, Bolei and Hu, Chengzhi and Weber-Genzel, Leon and R{\"o}ttger, Paul and Kreuter, Frauke and Hovy, Dirk and Plank, Barbara},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  title     = {{``}{M}y Answer is {C}{''}: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models},
  year      = {2024},
  address   = {Bangkok, Thailand},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  month     = aug,
  pages     = {7407--7416},
  publisher = {Association for Computational Linguistics},
  abstract  = {The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model{'}s diverse response styles such as starting with {``}Sure{''} or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned \textit{on all dimensions}, reaching mismatch rates over 60{\%}. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.},
  doi       = {10.18653/v1/2024.findings-acl.441},
  url       = {https://aclanthology.org/2024.findings-acl.441}
}

@inproceedings{Raina2023,
  author    = {Raina, Vatsal and Liusie, Adian and Gales, Mark},
  booktitle = {9th Workshop on Speech and Language Technology in Education (SLaTE)},
  title     = {Analyzing Multiple-Choice Reading and Listening Comprehension Tests},
  year      = {2023},
  month     = aug,
  pages     = {1--5},
  publisher = {ISCA},
  doi       = {10.21437/slate.2023-1}
}

@inproceedings{Byrd2022,
  author    = {Byrd, Matthew and Srivastava, Shashank},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {Predicting Difficulty and Discrimination of Natural Language Questions},
  year      = {2022},
  pages     = {119--130},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.acl-short.15}
}

@article{Chang2021,
  author    = {Chang, Hua-Hua and Wang, Chun and Zhang, Susu},
  journal   = {Annual Review of Statistics and Its Application},
  title     = {Statistical Applications in Educational Measurement},
  year      = {2021},
  issn      = {2326-831X},
  month     = mar,
  number    = {1},
  pages     = {439--461},
  volume    = {8},
  doi       = {10.1146/annurev-statistics-042720-104044},
  publisher = {Annual Reviews}
}

@article{Kurdi2019,
  author    = {Kurdi, Ghader and Leo, Jared and Parsia, Bijan and Sattler, Uli and Al-Emari, Salam},
  journal   = {International Journal of Artificial Intelligence in Education},
  title     = {A Systematic Review of Automatic Question Generation for Educational Purposes},
  year      = {2019},
  issn      = {1560-4306},
  month     = nov,
  number    = {1},
  pages     = {121--204},
  volume    = {30},
  doi       = {10.1007/s40593-019-00186-y},
  publisher = {Springer Science and Business Media LLC}
}

@article{Raina2022,
  author        = {Raina, Vatsal and Gales, Mark},
  title         = {Multiple-Choice Question Generation: Towards an Automated Assessment Framework},
  year          = {2022},
  month         = sep,
  abstract      = {Automated question generation is an important approach to enable personalisation of English comprehension assessment. Recently, transformer-based pretrained language models have demonstrated the ability to produce appropriate questions from a context paragraph. Typically, these systems are evaluated against a reference set of manually generated questions using n-gram based metrics, or manual qualitative assessment. Here, we focus on a fully automated multiple-choice question generation (MCQG) system where both the question and possible answers must be generated from the context paragraph. Applying n-gram based approaches is challenging for this form of system as the reference set is unlikely to capture the full range of possible questions and answer options. Conversely manual assessment scales poorly and is expensive for MCQG system development. In this work, we propose a set of performance criteria that assess different aspects of the generated multiple-choice questions of interest. These qualities include: grammatical correctness, answerability, diversity and complexity. Initial systems for each of these metrics are described, and individually evaluated on standard multiple-choice reading comprehension corpora.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2209.11830},
  eprint        = {2209.11830},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv}
}

@inproceedings{Park2024,
  author    = {Park, Jae-Woo and Park, Seong-Jin and Won, Hyun-Sik and Kim, Kang-Min},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  title     = {Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation},
  year      = {2024},
  pages     = {8157--8177},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2024.findings-emnlp.477}
}

@inproceedings{Lu2024,
  author     = {Lu, Xinyi and Wang, Xu},
  booktitle  = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
  title      = {Generative Students: Using {LLM}-Simulated Student Profiles to Support Question Item Evaluation},
  year       = {2024},
  month      = jul,
  pages      = {16--27},
  publisher  = {ACM},
  series     = {L@S ’24},
  collection = {L@S ’24},
  doi        = {10.1145/3657604.3662031}
}

@inproceedings{Plank2022,
  author    = {Plank, Barbara},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  title     = {The “Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation},
  year      = {2022},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.emnlp-main.731}
}

@inproceedings{Pezeshkpour2024,
  author    = {Pezeshkpour, Pouya and Hruschka, Estevam},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
  title     = {Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2024.findings-naacl.130}
}
